{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEerp9r124kK"
      },
      "outputs": [],
      "source": [
        "# Installs (run once per session)\n",
        "!pip install --upgrade pandas scikit-learn tqdm scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) (If using Drive) mount and set your CSV path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "CSV_PATH = '/content/drive/MyDrive/aphasia/aphasia_tokens.csv'\n",
        "SAVE_DIR = '/content/drive/MyDrive/aphasia'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPXkxGy62_41",
        "outputId": "cf92d316-cefb-49df-810e-4b9192f4fae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Imports ----\n",
        "import re, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy import sparse\n",
        "import os, datetime\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from joblib import dump, load"
      ],
      "metadata": {
        "id": "mbXUuZUl3F7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Load data ----\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "assert {'sample_id','token','is_word','is_CIU'}.issubset(df.columns), \"CSV must have columns: sample_id, token, is_word, is_CIU\"\n",
        "\n",
        "STAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "metadata": {
        "id": "cuP62LWX3PJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Build context window (prev/next) per token ----\n",
        "def add_context(df):\n",
        "    rows = []\n",
        "    for sid, g in df.groupby('sample_id', sort=False):\n",
        "        toks = g['token'].tolist()\n",
        "        isw  = g['is_word'].astype(int).tolist()\n",
        "        iciu = g['is_CIU' ].astype(int).tolist()\n",
        "        n = len(toks)\n",
        "        for i in range(n):\n",
        "            rows.append({\n",
        "                'sample_id': sid,\n",
        "                'token': toks[i] if isinstance(toks[i], str) else str(toks[i]),\n",
        "                'prev1': toks[i-1] if i-1 >= 0 else '<BOS>',\n",
        "                'next1': toks[i+1] if i+1 < n else '<EOS>',\n",
        "                'prev2': toks[i-2] if i-2 >= 0 else '<BOS2>',\n",
        "                'next2': toks[i+2] if i+2 < n else '<EOS2>',\n",
        "                'is_word': isw[i],\n",
        "                'is_CIU' : iciu[i],\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "X_all = add_context(df)"
      ],
      "metadata": {
        "id": "k5S1u5it3RoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = {\n",
        "    'the','a','an','and','but','or','so','to','of','in','on','at','for','from','with','by','as',\n",
        "    'that','this','these','those','it','its','is','was','are','were','be','been','being','do','does','did',\n",
        "    'have','has','had','he','she','they','we','you','i','me','him','her','them','us','my','your','our',\n",
        "    'their','his','hers','theirs','mine','yours','ours','not','no','if','then','because','about','over',\n",
        "    'under','up','down','out','into','off','just','very','really','there','here','now','also','too','again'\n",
        "}\n",
        "FILLERS = {'uh','um','er','ah','oh','mm','hmm','yeah','yep','nope','okay','ok','alright'}\n",
        "def safe_str(x):\n",
        "    return x if isinstance(x, str) else str(x)"
      ],
      "metadata": {
        "id": "vROwRtCB3Utj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HandcraftedFeats(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, use_context=True):\n",
        "        self.use_context = use_context\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def _feat_row(self, tok, prev1, next1):\n",
        "        t = safe_str(tok)\n",
        "        p = safe_str(prev1)\n",
        "        n = safe_str(next1)\n",
        "\n",
        "        def counts(s):\n",
        "            s = safe_str(s)\n",
        "            al = sum(ch.isalpha() for ch in s)\n",
        "            di = sum(ch.isdigit() for ch in s)\n",
        "            pu = sum((not ch.isalnum()) for ch in s)\n",
        "            return al, di, pu\n",
        "\n",
        "        len_t = len(t)\n",
        "        alpha_t, digit_t, punct_t = counts(t)\n",
        "\n",
        "        feats = [\n",
        "            len_t,\n",
        "            alpha_t,\n",
        "            digit_t,\n",
        "            punct_t,\n",
        "            1.0 if t.isalpha() else 0.0,\n",
        "            1.0 if t.islower() else 0.0,\n",
        "            1.0 if t.isupper() else 0.0,\n",
        "            1.0 if \"'\" in t else 0.0,\n",
        "            1.0 if \"-\" in t or \"â€“\" in t else 0.0,\n",
        "            1.0 if any(ch.isdigit() for ch in t) else 0.0,\n",
        "            1.0 if t.lower() in STOPWORDS else 0.0,\n",
        "            1.0 if t.lower() in FILLERS else 0.0,\n",
        "            1.0 if t.lower() == 'and' else 0.0,\n",
        "            1.0 if t.lower().startswith('&=') else 0.0,\n",
        "            1.0 if t.lower() in {'xxx','xx'} else 0.0,\n",
        "            1.0 if t.endswith('-') else 0.0,\n",
        "        ]\n",
        "\n",
        "        if self.use_context:\n",
        "            feats += [\n",
        "                1.0 if safe_str(p).lower() in STOPWORDS else 0.0,\n",
        "                1.0 if safe_str(n).lower() in STOPWORDS else 0.0,\n",
        "                1.0 if safe_str(p).lower() in FILLERS else 0.0,\n",
        "                1.0 if safe_str(n).lower() in FILLERS else 0.0,\n",
        "            ]\n",
        "        return feats\n",
        "\n",
        "    def transform(self, X):\n",
        "        # X is a 2D array (subset of columns from ColumnTransformer)\n",
        "        # assume columns are [token, prev1, next1]\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            arr = X[['token','prev1','next1']].values\n",
        "        else:\n",
        "            arr = X\n",
        "        rows = [self._feat_row(tok, prev1, next1) for tok, prev1, next1 in arr]\n",
        "        return sparse.csr_matrix(np.asarray(rows, dtype=np.float32))\n",
        "\n",
        "# ---- Preprocessor: char n-grams + handcrafted features ----\n",
        "tok_char = TfidfVectorizer(analyzer='char', ngram_range=(2,5), lowercase=True, min_df=1)\n",
        "ctx_prev = TfidfVectorizer(analyzer='char', ngram_range=(2,5), lowercase=True, min_df=1)\n",
        "ctx_next = TfidfVectorizer(analyzer='char', ngram_range=(2,5), lowercase=True, min_df=1)\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('tok_char', tok_char, 'token'),\n",
        "        ('prev_char', ctx_prev, 'prev1'),\n",
        "        ('next_char', ctx_next, 'next1'),\n",
        "        ('hand', HandcraftedFeats(), ['token','prev1','next1']),\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    sparse_threshold=1.0\n",
        ")\n",
        "\n",
        "# ---- Build the DataFrame of inputs and targets ----\n",
        "X_df = X_all[['sample_id','token','prev1','next1']].copy()\n",
        "y_word = X_all['is_word'].astype(int).values\n",
        "y_ciu  = X_all['is_CIU' ].astype(int).values\n",
        "groups = X_all['sample_id'].values\n",
        "\n",
        "# ---- Grouped train/test split by sample_id (80/20) ----\n",
        "SEED = 42\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
        "(train_idx, test_idx), = gss.split(X_df, y_word, groups=groups)\n",
        "\n",
        "X_train, X_test = X_df.iloc[train_idx], X_df.iloc[test_idx]\n",
        "y_word_tr, y_word_te = y_word[train_idx], y_word[test_idx]\n",
        "y_ciu_tr,  y_ciu_te  = y_ciu [train_idx], y_ciu [test_idx]\n",
        "groups_tr, groups_te = groups[train_idx], groups[test_idx]\n",
        "\n",
        "print(f\"Train tokens: {len(X_train)} | Test tokens: {len(X_test)}\")\n",
        "print(f\"Train samples: {X_train['sample_id'].nunique()} | Test samples: {X_test['sample_id'].nunique()}\")\n",
        "\n",
        "# ---- Models to train ----\n",
        "def model_zoo():\n",
        "    return {\n",
        "        'SVM-linear':  SVC(kernel='linear', probability=True, class_weight='balanced', random_state=SEED),\n",
        "        'SVM-rbf':     SVC(kernel='rbf',   probability=True, class_weight='balanced', C=2.0, gamma='scale', random_state=SEED),\n",
        "        'RandomForest': RandomForestClassifier(\n",
        "            n_estimators=300, max_depth=None, class_weight='balanced_subsample', n_jobs=-1, random_state=SEED\n",
        "        ),\n",
        "        'DecisionTree': DecisionTreeClassifier(max_depth=None, class_weight='balanced', random_state=SEED),\n",
        "        'KNN':          KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
        "    }\n",
        "\n",
        "# ---- Training/Evaluation helper ----\n",
        "def train_eval(task_name, y_tr, y_te):\n",
        "    results = []\n",
        "    saved_paths = []\n",
        "    for name, clf in model_zoo().items():\n",
        "        pipe = Pipeline(steps=[\n",
        "            ('prep', preprocessor),\n",
        "            # Note: TF-IDF gives sparse normalized features; numeric feats remain sparse via HandcraftedFeats\n",
        "            ('clf', clf)\n",
        "        ])\n",
        "        pipe.fit(X_train[['token','prev1','next1']], y_tr)\n",
        "\n",
        "        # --- SAVE the fitted pipeline ---\n",
        "        model_path = os.path.join(SAVE_DIR, f\"{task_name}_{name}_{STAMP}.joblib\")\n",
        "        dump(pipe, model_path, compress=(\"xz\", 3))\n",
        "        saved_paths.append(model_path)\n",
        "\n",
        "        # Evaluate\n",
        "        yhat = pipe.predict(X_test[['token','prev1','next1']])\n",
        "        # scores for ROC-AUC\n",
        "        if hasattr(pipe.named_steps['clf'], 'predict_proba'):\n",
        "            yscore = pipe.predict_proba(X_test[['token','prev1','next1']])[:,1]\n",
        "        elif hasattr(pipe.named_steps['clf'], 'decision_function'):\n",
        "            dfun = pipe.decision_function(X_test[['token','prev1','next1']])\n",
        "            # rescale to [0,1] to be safe\n",
        "            dfun = (dfun - dfun.min()) / (dfun.max() - dfun.min() + 1e-9)\n",
        "            yscore = dfun\n",
        "        else:\n",
        "            yscore = (yhat == 1).astype(float)\n",
        "\n",
        "        acc = accuracy_score(y_te, yhat)\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(y_te, yhat, average='binary', zero_division=0)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_te, yscore)\n",
        "        except Exception:\n",
        "            auc = float(\"nan\")\n",
        "        cm = confusion_matrix(y_te, yhat)\n",
        "\n",
        "        print(f\"\\n[{task_name} | {name}]  Acc={acc:.4f}  Prec={prec:.4f}  Rec={rec:.4f}  F1={f1:.4f}  AUC={auc:.4f}\")\n",
        "        print(\"Confusion matrix [[TN FP],[FN TP]]:\\n\", cm)\n",
        "\n",
        "        results.append({\n",
        "            'task': task_name, 'model': name,\n",
        "            'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'auc': auc\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n=== WORD vs NOT_WORD ===\")\n",
        "res_word = train_eval('WORD', y_word_tr, y_word_te)\n",
        "\n",
        "print(\"\\n=== CIU vs NOT_CIU ===\")\n",
        "res_ciu  = train_eval('CIU',  y_ciu_tr,  y_ciu_te)\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "summary = pd.concat([res_word, res_ciu], ignore_index=True)\n",
        "summary.sort_values(['task','f1'], ascending=[True, False], inplace=True)\n",
        "summary.reset_index(drop=True, inplace=True)\n",
        "summary\n",
        "\n",
        "metrics_path = os.path.join(SAVE_DIR, f\"classifier_metrics_{STAMP}.csv\")\n",
        "summary.to_csv(metrics_path, index=False)\n",
        "print(\"Metrics saved to:\", metrics_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQOyUx5E3tzH",
        "outputId": "3851148d-4e96-4b36-cacf-7af8a3e0c889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tokens: 1755 | Test tokens: 222\n",
            "Train samples: 28 | Test samples: 7\n",
            "\n",
            "=== WORD vs NOT_WORD ===\n",
            "\n",
            "[WORD | SVM-linear]  Acc=0.9955  Prec=0.9954  Rec=1.0000  F1=0.9977  AUC=0.9946\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[  5   1]\n",
            " [  0 216]]\n",
            "\n",
            "[WORD | SVM-rbf]  Acc=0.9955  Prec=0.9954  Rec=1.0000  F1=0.9977  AUC=0.9969\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[  5   1]\n",
            " [  0 216]]\n",
            "\n",
            "[WORD | RandomForest]  Acc=0.9955  Prec=0.9954  Rec=1.0000  F1=0.9977  AUC=0.9892\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[  5   1]\n",
            " [  0 216]]\n",
            "\n",
            "[WORD | DecisionTree]  Acc=0.9955  Prec=0.9954  Rec=1.0000  F1=0.9977  AUC=0.9167\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[  5   1]\n",
            " [  0 216]]\n",
            "\n",
            "[WORD | KNN]  Acc=0.9955  Prec=0.9954  Rec=1.0000  F1=0.9977  AUC=0.9136\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[  5   1]\n",
            " [  0 216]]\n",
            "\n",
            "=== CIU vs NOT_CIU ===\n",
            "\n",
            "[CIU | SVM-linear]  Acc=0.7883  Prec=0.8678  Rec=0.8629  F1=0.8653  AUC=0.7463\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[ 24  23]\n",
            " [ 24 151]]\n",
            "\n",
            "[CIU | SVM-rbf]  Acc=0.8063  Prec=0.8750  Rec=0.8800  F1=0.8775  AUC=0.7973\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[ 25  22]\n",
            " [ 21 154]]\n",
            "\n",
            "[CIU | RandomForest]  Acc=0.7928  Prec=0.8772  Rec=0.8571  F1=0.8671  AUC=0.7660\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[ 26  21]\n",
            " [ 25 150]]\n",
            "\n",
            "[CIU | DecisionTree]  Acc=0.6982  Prec=0.8506  Rec=0.7486  F1=0.7964  AUC=0.6282\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[ 24  23]\n",
            " [ 44 131]]\n",
            "\n",
            "[CIU | KNN]  Acc=0.8243  Prec=0.8864  Rec=0.8914  F1=0.8889  AUC=0.7870\n",
            "Confusion matrix [[TN FP],[FN TP]]:\n",
            " [[ 27  20]\n",
            " [ 19 156]]\n",
            "\n",
            "=== Summary ===\n",
            "Metrics saved to: /content/drive/MyDrive/aphasia/classifier_metrics_20250818-142253.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0glZ1dnGiu8_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}